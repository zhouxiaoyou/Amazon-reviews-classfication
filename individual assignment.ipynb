{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from time import time\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB,GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from scipy.sparse import csr_matrix,coo_matrix,csc_matrix\n",
    "import scipy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction, task, and data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is well known that customers' reviews are important information. This assignment aims to develop predictive models for classifying the good or bad reviews, and improve models(accuracy,F-score,etc) by features decomposition and different classifiers. In the last part,explore why some reviews were misclassified and analyze the error samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Organize and split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "fn = \".../input_data/Beauty_5.json\"\n",
    "reviews = (eval(line) for line in open(fn))\n",
    "reviews = pd.DataFrame(reviews)\n",
    "#Giving labels to reviews \n",
    "#Assume reviews' ratings that over than 4.5 are 'good' reviews\n",
    "reviews['good']= reviews.overall > 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train data(70%),test data(20%),evaluation data(10%)\n",
    "train_data, test_data = train_test_split(reviews, test_size=0.3, random_state=1)\n",
    "test_data, val_data = train_test_split(test_data ,test_size=0.33,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198502, 138951, 39899, 19652)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the size of the data sets\n",
    "len(reviews),len(train_data),len(test_data),len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.57607357989507091, 0.57921251159176923, 0.57882149399552207)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label distribution \n",
    "sum(train_data.good)/len(train_data),sum(test_data.good)/len(test_data),sum(val_data.good)/len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split all data into train data(70%),test data(20%),evaluation data(10%),the good labels accounted for about 57% in three data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Baseline model & performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of baseline model (predict good labels) is 57.92%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create document term matrix of reviews\n",
    "vectorizer = TfidfVectorizer(input='content')\n",
    "x_train = vectorizer.fit_transform(train_data.reviewText)  \n",
    "x_test = vectorizer.transform(test_data.reviewText)\n",
    "y_train = train_data.good\n",
    "y_test = test_data.good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time：0.099s\n",
      "test time：0.022s\n",
      "accuracy：74.84%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.84      0.50      0.62     16789\n",
      "       Good       0.72      0.93      0.81     23110\n",
      "\n",
      "avg / total       0.77      0.75      0.73     39899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fit MultinomialNB\n",
    "time_star = time()\n",
    "clf = MultinomialNB().fit(x_train,y_train)\n",
    "time_end = time()\n",
    "train_time = time_end-time_star \n",
    "print (u'train time:%.3fs' % train_time)\n",
    "\n",
    "time_star = time()\n",
    "predictions = clf.predict(x_test)\n",
    "time_end = time()\n",
    "test_time = time_end-time_star \n",
    "print (u'test time:%.3fs' % test_time)\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, predictions)\n",
    "print (u'accuracy:%.2f%%' % (100 * acc))\n",
    "print(metrics.classification_report(y_test, predictions, target_names=[\"Bad\", \"Good\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of initial model is 74.84% which is higher than baseline model(57.92%).According to the classification report,this model is good at identifying true positive samples(recall = 0.93,f1-score = 0.81).However,this model is not good at identifying true negative samples(f1-score = 0.63)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Model improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1  feature improvements** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to add new feature(user mean scores)by using train data information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means =train_data.groupby(\"reviewerID\").overall.mean()\n",
    "train_data = pd.merge(train_data, pd.DataFrame(means),left_on=\"reviewerID\", right_index=True)\n",
    "test_data = pd.merge(test_data, pd.DataFrame(means),how = \"left\",left_on=\"reviewerID\", right_index=True)\n",
    "val_data = pd.merge(val_data, pd.DataFrame(means),how = \"left\",left_on=\"reviewerID\", right_index=True)\n",
    "#Handle null values\n",
    "val_data['overall_y'] = val_data['overall_y'].fillna(np.mean(train_data.overall_y))\n",
    "test_data['overall_y'] = test_data['overall_y'].fillna(np.mean(train_data.overall_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Converted to matrix\n",
    "mean_train = coo_matrix(train_data['overall_y'])\n",
    "mean_val = coo_matrix(val_data['overall_y'])\n",
    "mean_test = coo_matrix(test_data['overall_y'])\n",
    "#conbine dtm features and mean scores features\n",
    "features_train = scipy.sparse.hstack([mean_train.T,train_data.reviewText])\n",
    "features_val = scipy.sparse.hstack([mean_val.T,train_data.reviewText])\n",
    "features_test = scipy.sparse.hstack([mean_test.T,train_data.reviewText])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try an classifier MultinomialNB and compared with initial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy：70.49%\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(features_train,y_train)\n",
    "predictions = clf.predict(features_val)\n",
    "acc = metrics.accuracy_score(y_val, predictions)\n",
    "print (u'accuracy:%.2f%%' % (100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the average score of users lead to reduce the accuracy compared with the initial model. It may be because the user mean ratings have some noise data. So I decided to use another way to imporve features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firtly,Stop word removal,feature weighting to improve the feature of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input='content',max_df=0.5, sublinear_tf=True,use_idf=True)\n",
    "x_train = vectorizer.fit_transform(train_data.reviewText) \n",
    "x_val = vectorizer.transform(val_data.reviewText)\n",
    "x_test = vectorizer.transform(test_data.reviewText)\n",
    "y_train = train_data.good\n",
    "y_test = test_data.good\n",
    "y_val = val_data.good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly,Feature decomposition by linearsvc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "lsvc = LinearSVC(penalty=\"l1\",dual=False).fit(x_train,y_train)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "x_train_lsvc = model.transform(x_train)\n",
    "x_test_lsvc = model.transform(x_test)\n",
    "x_val_lsvc = model.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52572, 52572, 52572)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many features removed?\n",
    "x_train.shape[1]-x_train_lsvc.shape[1],x_test.shape[1]-x_test_lsvc.shape[1],x_val.shape[1]-x_val_lsvc.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Classifier selection**\n",
    "\n",
    "Considering the computation time, system memory and label characteristics, the linear model (eg.SGDClassifier,LogisticRegression) will be used in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly,Select best hyperparameter for SGDClassifier and LogisticRegression by GridSearchCV,then fit train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'], \n",
    "              'penalty':[\"l1\",\"l2\"],\n",
    "              'n_jobs': [-1,1],\n",
    "             }\n",
    "SGDC = SGDClassifier()\n",
    "gs = GridSearchCV(SGDC, parameters)\n",
    "gs.fit(x_train_lsvc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'dual':[False], \n",
    "              'penalty':[\"l2\"],\n",
    "              'n_jobs': [-1,1],\n",
    "              'C':[ 0.01,  0.21,  0.41,  0.61,  0.81,  1.01,  1.21,  1.41,  1.61,  1.81],\n",
    "              \"solver\":[\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\"]\n",
    "             }\n",
    "lr = LogisticRegression()\n",
    "gs_lr = GridSearchCV(lr, parameters)\n",
    "gs_lr.fit(x_train_lsvc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then,predict the evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg fit time：0.358s\n",
      "test time：0.003s\n",
      "accuracy：80.02%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.75      0.57      0.65      6293\n",
      "       Good       0.82      0.91      0.86     13359\n",
      "\n",
      "avg / total       0.79      0.80      0.79     19652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_star = time()\n",
    "predictions = gs.predict(x_val_lsvc)\n",
    "time_end = time()\n",
    "test_time = time_end-time_star\n",
    "\n",
    "acc = metrics.accuracy_score(y_val, predictions)\n",
    "\n",
    "print (u'avg fit time:%.3fs' % np.mean(gs.cv_results_['mean_fit_time']))\n",
    "print (u'test time:%.3fs' % test_time)\n",
    "print (u'accuracy:%.2f%%' % (100 * acc))\n",
    "print(metrics.classification_report(y_val, predictions, target_names=['Bad', 'Good']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg fit time：2.547s\n",
      "test time：0.003s\n",
      "accuracy：80.28%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.73      0.61      0.66      6293\n",
      "       Good       0.83      0.90      0.86     13359\n",
      "\n",
      "avg / total       0.80      0.80      0.80     19652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_star = time()\n",
    "predictions = gs_lr.predict(x_val_lsvc)\n",
    "time_end = time()\n",
    "test_time = time_end-time_star\n",
    "\n",
    "acc = metrics.accuracy_score(y_val, predictions)\n",
    "\n",
    "print (u'avg fit time:%.3fs' % np.mean(gs_lr.cv_results_['mean_fit_time']))\n",
    "print (u'test time:%.3fs' % test_time)\n",
    "print (u'accuracy:%.2f%%' % (100 * acc))\n",
    "print(metrics.classification_report(y_val, predictions, target_names=['Bad', 'Good']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the accuracy and the stability of the model, LogisticRegression (accuracy = 80.28%, avg f1-score = 0.80) is slightly higher than SGDClassifier (accuracy = 80.02%, avg f1-score = 0.79), but the fit time of SGDClassifier (0.358s) is shorter than logical regression (2.547s). Finally, I decided to choose the higher accuracy classifier(logical regression). Next,the LogisticRegression will be used to predict test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time：3.930s\n",
      "test time：0.007s\n",
      "accuracy：80.56%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.74      0.61      0.67     12893\n",
      "       Good       0.83      0.90      0.86     27006\n",
      "\n",
      "avg / total       0.80      0.81      0.80     39899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_star = time()\n",
    "clf = gs_lr.best_estimator_.fit(x_train_lsvc,y_train)\n",
    "time_end = time()\n",
    "train_time = time_end-time_star \n",
    "print (u'train time:%.3fs' % train_time)\n",
    "\n",
    "time_star = time()\n",
    "predictions = clf.predict(x_test_lsvc)\n",
    "time_end = time()\n",
    "test_time = time_end-time_star \n",
    "print (u'test time:%.3fs' % test_time)\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, predictions)\n",
    "print (u'accuracy:%.2f%%' % (100 * acc))\n",
    "print(metrics.classification_report(y_test, predictions, target_names=[\"Bad\", \"Good\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy and f1-score of LogisticRegression is 80.56%, which is higher than initial model(74.84%). Just like the initial model,this model is good at identifying true positive samples(recall = 0.93,f1-score = 0.81). Despite reduced dimension of features, the training time of LogisticRegression is longer than the initial model due to the complexity of the calculation. Compared with initial model,the accuracy rate increased about 6%,precision,recall and f1-score increased 3%,4%,and 7% respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Error analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of bad are predicted as good is higher than good are predicted as bad.Is there a mistake in label good and bad? The wrong samples will be analyzed to answer this question in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select wrong samples\n",
    "wrong_samples = test_data[np.logical_xor(test_data.good,predictions)][['reviewerName',\"overall_x\",\"summary\",\"reviewText\",\"good\",\"overall_y\"]]\n",
    "wrong_samples = wrong_samples.rename(columns={\"reviewerName\": \"Name\", \"overall_x\": \"overall\",'overall_y':'mean_overall'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1 For 'bad' labels are predicted as 'good'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>good</th>\n",
       "      <th>mean_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192896</th>\n",
       "      <td>*rose*</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Dove Pure Care Dry Oil Conditioner</td>\n",
       "      <td>This is a nice conditioner that works very wel...</td>\n",
       "      <td>False</td>\n",
       "      <td>4.093023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179479</th>\n",
       "      <td>*rose*</td>\n",
       "      <td>4.0</td>\n",
       "      <td>CoverGirl 300 Flamed Out Mascara, Very Black B...</td>\n",
       "      <td>I like this mascara.  When I'm done applying i...</td>\n",
       "      <td>False</td>\n",
       "      <td>4.093023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57798</th>\n",
       "      <td>*rose*</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Works - with a couple irritations</td>\n",
       "      <td>This works very well.  It goes on smoothly and...</td>\n",
       "      <td>False</td>\n",
       "      <td>4.093023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189651</th>\n",
       "      <td>*rose*</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Suave Professionals Natural Infusion Seaweed a...</td>\n",
       "      <td>I like this shampoo.  The scent is different f...</td>\n",
       "      <td>False</td>\n",
       "      <td>4.093023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30029</th>\n",
       "      <td>*rose*</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Made my hair feel so soft with big bouncy curls!</td>\n",
       "      <td>I love this!  Works great and is so easy to us...</td>\n",
       "      <td>False</td>\n",
       "      <td>4.093023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189851</th>\n",
       "      <td>*rose*</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Suave Professionals Split End Rescue Conditioner</td>\n",
       "      <td>We use this conditioner with the Suave Split E...</td>\n",
       "      <td>False</td>\n",
       "      <td>4.093023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165330</th>\n",
       "      <td>*rose*</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Suave Professionals Moroccan Infusion Shine Sh...</td>\n",
       "      <td>This is a very luxurious shampoo.  It lathers ...</td>\n",
       "      <td>False</td>\n",
       "      <td>4.093023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name  overall                                            summary  \\\n",
       "192896  *rose*      4.0                 Dove Pure Care Dry Oil Conditioner   \n",
       "179479  *rose*      4.0  CoverGirl 300 Flamed Out Mascara, Very Black B...   \n",
       "57798   *rose*      4.0                  Works - with a couple irritations   \n",
       "189651  *rose*      4.0  Suave Professionals Natural Infusion Seaweed a...   \n",
       "30029   *rose*      4.0   Made my hair feel so soft with big bouncy curls!   \n",
       "189851  *rose*      4.0   Suave Professionals Split End Rescue Conditioner   \n",
       "165330  *rose*      3.0  Suave Professionals Moroccan Infusion Shine Sh...   \n",
       "\n",
       "                                               reviewText   good  mean_overall  \n",
       "192896  This is a nice conditioner that works very wel...  False      4.093023  \n",
       "179479  I like this mascara.  When I'm done applying i...  False      4.093023  \n",
       "57798   This works very well.  It goes on smoothly and...  False      4.093023  \n",
       "189651  I like this shampoo.  The scent is different f...  False      4.093023  \n",
       "30029   I love this!  Works great and is so easy to us...  False      4.093023  \n",
       "189851  We use this conditioner with the Suave Split E...  False      4.093023  \n",
       "165330  This is a very luxurious shampoo.  It lathers ...  False      4.093023  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_samples[wrong_samples.Name =='*rose*']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This user is a typical representative. In her opinion 4 stars means that the product is very good, so her comments have many words like \"like\", \"good\". Therefore, the standard of the good labels(>= 4.5)leads to wrong classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2 For 'good' labels are predicted as 'bad'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>good</th>\n",
       "      <th>mean_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91760</th>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Not sure if it works for me</td>\n",
       "      <td>I received this very quickly from the supplier...</td>\n",
       "      <td>True</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14438</th>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Yep, it's acetone</td>\n",
       "      <td>Not much to say about this product except that...</td>\n",
       "      <td>True</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94180</th>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Does not cause flaked/peeling skin (for me)</td>\n",
       "      <td>I didn't have to worry about peeling. The inst...</td>\n",
       "      <td>True</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75360</th>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>5.0</td>\n",
       "      <td>keeps depression at bay</td>\n",
       "      <td>I've been using Nioxin since my hair started f...</td>\n",
       "      <td>True</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name  overall                                      summary  \\\n",
       "91760  Amazon Customer      5.0                  Not sure if it works for me   \n",
       "14438  Amazon Customer      5.0                            Yep, it's acetone   \n",
       "94180  Amazon Customer      5.0  Does not cause flaked/peeling skin (for me)   \n",
       "75360  Amazon Customer      5.0                      keeps depression at bay   \n",
       "\n",
       "                                              reviewText  good  mean_overall  \n",
       "91760  I received this very quickly from the supplier...  True          4.50  \n",
       "14438  Not much to say about this product except that...  True          3.75  \n",
       "94180  I didn't have to worry about peeling. The inst...  True          4.25  \n",
       "75360  I've been using Nioxin since my hair started f...  True          5.00  "
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_bad = wrong_samples.ix[[91760,14438,94180,75360]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compound': 0.2732, 'neu': 0.952, 'pos': 0.048, 'neg': 0.0}\n",
      "{'compound': 0.2617, 'neu': 0.961, 'pos': 0.039, 'neg': 0.0}\n",
      "{'compound': 0.4025, 'neu': 0.955, 'pos': 0.033, 'neg': 0.012}\n",
      "{'compound': -0.1531, 'neu': 0.918, 'pos': 0.0, 'neg': 0.082}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "for i in good_bad['reviewText']:\n",
    "    alz = SentimentIntensityAnalyzer()\n",
    "    res = alz.polarity_scores(i)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to do Sentiment anaysis for these misclassified samples,It is clear that these comments are very neutral, and even have some negative words, which means that some customers have complained about the product, but still give 5 star."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the error analysis, I think the effectiveness of LogisticRegression is fine compared with initial model. There are some limitations in this assignment. \n",
    "\n",
    "Firstly,the standard of the good labels may mismatch the review texts.As I mentioned in the error analysis,the standard of good products is very subjective,so it is not accurate to label reviews by using rating >= 4.5, more information needed to create the standard of good reviews.\n",
    "\n",
    "Secondly,Due to memory limitations, some complex classifiers are not tried. Complex algorithms may have a better performance on text classify(e.g.SVM, Neural Networks),but these classifiers need computers with high standard configuration.\n",
    "\n",
    "Finally,More useful features should be added. For exmaple,user's average score may be a good feature, but we should handle the noise data; On the other hand, brands,text length etc also can as features.\n",
    "\n",
    "In sum, the prediction model trained in this assignment has room for improvement,Once has a good standard of classify good/bad reviews and different classifiers can uesed, this can prove a very robust method of prediecting reviews."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
